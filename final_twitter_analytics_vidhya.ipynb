{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_twitter_analytics_vidhya.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/shalini0289/Colab/blob/master/final_twitter_analytics_vidhya.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "trY_Bel3f_5T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "3154743b-97ad-4a4f-9614-726b3e5143ce"
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 4954496233748577667]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 161
        }
      ]
    },
    {
      "metadata": {
        "id": "BZE09f3qgEtX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "import io , requests\n",
        "import sys\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4USw48WygTlc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "drive_service = build('drive', 'v3')\n",
        "SOURCE_FOLDER=''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eCiQWUkegZNP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_parent_folder(folder_name):\n",
        "  page_token = None\n",
        "  folder_array = []\n",
        "  query = \"name='%s' and mimeType='application/vnd.google-apps.folder'\" % folder_name\n",
        "  while True:\n",
        "      response = drive_service.files().list(q=query,\n",
        "                                          spaces='drive',\n",
        "                                          fields='nextPageToken, files(id, name)',\n",
        "                                          pageToken=page_token).execute()\n",
        "      for file in response.get('files', []):\n",
        "          # Process change\n",
        "          #print (file.get('name'), file.get('id'))\n",
        "          folder_array.append({\"name\" : file.get('name'), \"id\" : file.get('id')})\n",
        "      page_token = response.get('nextPageToken', None)\n",
        "      if page_token is None:\n",
        "          break\n",
        "  return folder_array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k8RczsX5ggZp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_files_from_parent(parent_id):\n",
        "  page_token = None\n",
        "  folder_array = dict()\n",
        "  query = \"'%s' in parents\" % parent_id\n",
        "  while True:\n",
        "      response = drive_service.files().list(q=query,\n",
        "                                          spaces='drive',\n",
        "                                          fields='nextPageToken, files(id, name)',\n",
        "                                          pageToken=page_token).execute()\n",
        "      for file in response.get('files', []):\n",
        "          # Process change\n",
        "          #print (file.get('name'), file.get('id'))\n",
        "          folder_array.update({file.get('name'):file.get('id')})\n",
        "      page_token = response.get('nextPageToken', None)\n",
        "      if page_token is None:\n",
        "          break\n",
        "  return folder_array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gXjSwb4DgheR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_file_buffer(file_id, verbose=0):\n",
        "  from googleapiclient.http import MediaIoBaseDownload\n",
        "  request = drive_service.files().get_media(fileId=file_id)\n",
        "  downloaded = io.BytesIO()\n",
        "  downloader = MediaIoBaseDownload(downloaded, request)\n",
        "  done = False\n",
        "  while done is False:\n",
        "    # _ is a placeholder for a progress object that we ignore.\n",
        "    # (Our file is small, so we skip reporting progress.)\n",
        "    progress, done = downloader.next_chunk()\n",
        "    if verbose:\n",
        "      sys.stdout.flush()\n",
        "      sys.stdout.write('\\r')\n",
        "      percentage_done = progress.resumable_progress * 100/progress.total_size\n",
        "      sys.stdout.write(\"[%-100s] %d%%\" % ('='*int(percentage_done), int(percentage_done)))\n",
        "  downloaded.seek(0)\n",
        "  return downloaded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mT_Drig0glhE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e884078e-9a7c-4674-c28b-2eadcf73cf13"
      },
      "cell_type": "code",
      "source": [
        "parent_folder = get_parent_folder('av-sa')\n",
        "print(parent_folder)\n",
        "parent_folder[0][\"id\"]"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'name': 'av-sa', 'id': '1uOG8csXYChpdzul_0v5lPgY-N5ssebG5'}]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1uOG8csXYChpdzul_0v5lPgY-N5ssebG5'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 167
        }
      ]
    },
    {
      "metadata": {
        "id": "W5OKyYNygoXj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8d5acd59-f972-4a05-8d74-b8fb49b151e2"
      },
      "cell_type": "code",
      "source": [
        "input_file_meta = get_files_from_parent(parent_folder[0][\"id\"])\n",
        "print(input_file_meta)"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'test_tweets_anuFYb8.csv': '1DXtnnPd4THLQj6kUAU2JODxaQd3r1EXh', 'train_E6oV3lV.csv': '1FO1N6elMtXjPAh1Sz6BHojgHS-tqM8U3'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MRAfaCThgxYb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "19ef1878-9b00-4bf4-dfef-df2d554dc86f"
      },
      "cell_type": "code",
      "source": [
        "for file, id in input_file_meta.items():\n",
        "  downloaded = get_file_buffer(id, verbose=1)\n",
        "  dest_file = os.path.join(SOURCE_FOLDER, file)\n",
        "  print(\"processing %s data\" % file)\n",
        "  with open(dest_file, \"wb\") as out:\n",
        "    out.write(downloaded.read())\n",
        "    print(\"Done %s\" % dest_file)"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r[====================================================================================================] 100%processing test_tweets_anuFYb8.csv data\n",
            "Done test_tweets_anuFYb8.csv\n",
            "[====================================================================================================] 100%processing train_E6oV3lV.csv data\n",
            "Done train_E6oV3lV.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dapH7p53g2Cy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train=pd.read_csv(os.path.join(SOURCE_FOLDER,'train_E6oV3lV.csv'))\n",
        "test=pd.read_csv(os.path.join(SOURCE_FOLDER,'test_tweets_anuFYb8.csv'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BvDUkNmwg9sa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def remove_pattern(input_txt, pattern):\n",
        "    r = re.findall(pattern, input_txt)\n",
        "    for i in r:\n",
        "        input_txt = re.sub(i, '', input_txt)\n",
        "        \n",
        "    return input_txt   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UShjggtPhhoV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "ce4d3ff1-21f7-4eff-ace4-3f8bdc5a79d8"
      },
      "cell_type": "code",
      "source": [
        "!pip install wordcloud\n",
        "!pip install gensim\n"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.6/dist-packages (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from wordcloud) (1.14.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from wordcloud) (4.0.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->wordcloud) (0.45.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.5.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.6.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (0.19.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.5)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.9.0)\n",
            "Requirement already satisfied: bz2file in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (0.98)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.18.4)\n",
            "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.1.13)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.12.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.3)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.22)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2018.8.24)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.0->boto3->smart-open>=1.2.1->gensim) (0.14)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.0->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gV-9u5gthdNv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns\n",
        "import string\n",
        "import nltk\n",
        "import warnings \n",
        "import gensim\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import utils\n",
        "from gensim.test.utils import common_texts, get_tmpfile\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument, LabeledSentence\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yyOejIt-g_jO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# remove twitter handles (@user)\n",
        "# @[\\w]* -  a regular expression which will pick any word starting with ‘@\n",
        "\n",
        "train['tidy_tweet'] = np.vectorize(remove_pattern)(train['tweet'], \"@[\\w]*\")\n",
        "train = train.fillna(method='ffill')\n",
        "\n",
        "test['tidy_tweet'] = np.vectorize(remove_pattern)(test['tweet'], \"@[\\w]*\")\n",
        "test = test.fillna(method='ffill')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2tqtzquziGyu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# remove special characters, numbers, punctuations\n",
        "# replace everything except characters and hashtags with spaces\n",
        "\n",
        "train['tidy_tweet'] = train['tidy_tweet'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
        "test['tidy_tweet'] = test['tidy_tweet'].str.replace(\"[^a-zA-Z#]\", \" \")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dLhQ-BEUiOab",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#remove short words like : oh, hm etc\n",
        "\n",
        "train['tidy_tweet'] = train['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
        "test['tidy_tweet'] = test['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wlj5VuMgiXg1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "4a715805-a76a-4e23-c7f5-1b5a09f11c4a"
      },
      "cell_type": "code",
      "source": [
        "# splitting a string of text into tokens(individual terms or words)\n",
        "\n",
        "train_tokenized_tweet = train['tidy_tweet'].apply(lambda x: x.split())\n",
        "train_tokenized_tweet.head()"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [when, father, dysfunctional, selfish, drags, ...\n",
              "1    [thanks, #lyft, credit, cause, they, offer, wh...\n",
              "2                              [bihday, your, majesty]\n",
              "3                     [#model, love, take, with, time]\n",
              "4                   [factsguide, society, #motivation]\n",
              "Name: tidy_tweet, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 177
        }
      ]
    },
    {
      "metadata": {
        "id": "4d4Fqqg5ifcz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "392caaa0-1691-4705-905b-b5337a9e4e81"
      },
      "cell_type": "code",
      "source": [
        "# splitting a string of text into tokens(individual terms or words)\n",
        "\n",
        "test_tokenized_tweet = test['tidy_tweet'].apply(lambda x: x.split())\n",
        "test_tokenized_tweet.head()"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [#studiolife, #aislife, #requires, #passion, #...\n",
              "1    [#white, #supremacists, want, everyone, #birds...\n",
              "2    [safe, ways, heal, your, #acne, #altwaystoheal...\n",
              "3    [cursed, child, book, reservations, already, w...\n",
              "4    [#bihday, amazing, hilarious, #nephew, ahmir, ...\n",
              "Name: tidy_tweet, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 178
        }
      ]
    },
    {
      "metadata": {
        "id": "hcRaz6ikimck",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Stemming is a rule-based process of stripping the suffixes (“ing”, “ly”, “es”, “s” etc)\n",
        "\n",
        "from nltk.stem.porter import *\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "train_tokenized_tweet = train_tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
        "test_tokenized_tweet = test_tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nz5E8pw6izp4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#combine tokens\n",
        "for i in range(len(train_tokenized_tweet)):\n",
        " train_tokenized_tweet[i] = \",\".join(train_tokenized_tweet[i])\n",
        "train['tidy_tweet'] = train_tokenized_tweet\n",
        "\n",
        "for i in range(len(test_tokenized_tweet)):\n",
        " test_tokenized_tweet[i] = \",\".join(test_tokenized_tweet[i])\n",
        "test['tidy_tweet'] = test_tokenized_tweet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FO_GwFrHm6VY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data, valid_data = train_test_split(train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TomZqJsNxgfu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tagged = train_data.apply(lambda r: TaggedDocument(words=gensim.utils.simple_preprocess(r['tidy_tweet']), tags=[r.label]), axis=1)\n",
        "valid_tagged = valid_data.apply(lambda r: TaggedDocument(words=gensim.utils.simple_preprocess(r['tidy_tweet']), tags=[r.label]), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xCDi01sboUUt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def vec_for_learning(doc2vec_model, tagged_docs):\n",
        "    sents = tagged_docs.values\n",
        "    targets, regressors = zip(*[(doc.tags[0], doc2vec_model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
        "    return targets, regressors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5f5PhyRdokrX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#cores = multiprocessing.cpu_count()\n",
        "#model = Doc2Vec(tagged.values, vector_size=100, window=10, epoch=40, dm=1)\n",
        "model = Doc2Vec(tagged.values,  dm=1, epoch=20, vector_size=100, window=2, negative=5, min_count=2, workers=4, alpha=0.065, min_alpha=0.0651)  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jeELxnUHFyig",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# model.save('mc_twitter.d2v')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aAJCYjQP7N7m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d1aed032-2098-4f66-db67-ca890ce85aa6"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "y_train, X_train = vec_for_learning(model, tagged)\n",
        "y_valid, X_valid = vec_for_learning(model, valid_tagged)\n",
        "\n",
        "logreg = LogisticRegression()\n",
        "logreg.fit(X_train, y_train)\n",
        "y_pred = logreg.predict(X_valid)\n",
        "f1_score(y_valid, y_pred, average='macro')"
      ],
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7925361416931067"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 199
        }
      ]
    },
    {
      "metadata": {
        "id": "YHxBnG0pFowj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_tagged = test.apply(lambda r: TaggedDocument(words=gensim.utils.simple_preprocess(r['tidy_tweet']), tags=[]), axis=1)\n",
        "sents = test_tagged.values\n",
        "X_test = [(model.infer_vector(doc.words, steps=20)) for doc in sents]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9CtYkBVZ8zgi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_test = logreg.predict(X_test)\n",
        "test['label'] = y_test\n",
        "submission = test[['id','label']]\n",
        "my_sub = os.path.join(SOURCE_FOLDER, \"sub_twitter.csv\")\n",
        "submission.to_csv(my_sub, index=False) # writing data to a CSV file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dG4tFE78DTaM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download( \"sub_twitter.csv\" )"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}