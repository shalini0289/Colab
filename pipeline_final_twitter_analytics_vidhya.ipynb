{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "av_final_twitter_analytics_vidhya.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/shalini0289/Colab/blob/master/pipeline_final_twitter_analytics_vidhya.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "trY_Bel3f_5T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "fe8fdd52-62dc-43bf-d895-ff66ec2eed8c"
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 473590152623840415]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "BZE09f3qgEtX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "import io , requests\n",
        "import sys\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4USw48WygTlc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "drive_service = build('drive', 'v3')\n",
        "SOURCE_FOLDER=''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eCiQWUkegZNP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_parent_folder(folder_name):\n",
        "  page_token = None\n",
        "  folder_array = []\n",
        "  query = \"name='%s' and mimeType='application/vnd.google-apps.folder'\" % folder_name\n",
        "  while True:\n",
        "      response = drive_service.files().list(q=query,\n",
        "                                          spaces='drive',\n",
        "                                          fields='nextPageToken, files(id, name)',\n",
        "                                          pageToken=page_token).execute()\n",
        "      for file in response.get('files', []):\n",
        "          # Process change\n",
        "          #print (file.get('name'), file.get('id'))\n",
        "          folder_array.append({\"name\" : file.get('name'), \"id\" : file.get('id')})\n",
        "      page_token = response.get('nextPageToken', None)\n",
        "      if page_token is None:\n",
        "          break\n",
        "  return folder_array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k8RczsX5ggZp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_files_from_parent(parent_id):\n",
        "  page_token = None\n",
        "  folder_array = dict()\n",
        "  query = \"'%s' in parents\" % parent_id\n",
        "  while True:\n",
        "      response = drive_service.files().list(q=query,\n",
        "                                          spaces='drive',\n",
        "                                          fields='nextPageToken, files(id, name)',\n",
        "                                          pageToken=page_token).execute()\n",
        "      for file in response.get('files', []):\n",
        "          # Process change\n",
        "          #print (file.get('name'), file.get('id'))\n",
        "          folder_array.update({file.get('name'):file.get('id')})\n",
        "      page_token = response.get('nextPageToken', None)\n",
        "      if page_token is None:\n",
        "          break\n",
        "  return folder_array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gXjSwb4DgheR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_file_buffer(file_id, verbose=0):\n",
        "  from googleapiclient.http import MediaIoBaseDownload\n",
        "  request = drive_service.files().get_media(fileId=file_id)\n",
        "  downloaded = io.BytesIO()\n",
        "  downloader = MediaIoBaseDownload(downloaded, request)\n",
        "  done = False\n",
        "  while done is False:\n",
        "    # _ is a placeholder for a progress object that we ignore.\n",
        "    # (Our file is small, so we skip reporting progress.)\n",
        "    progress, done = downloader.next_chunk()\n",
        "    if verbose:\n",
        "      sys.stdout.flush()\n",
        "      sys.stdout.write('\\r')\n",
        "      percentage_done = progress.resumable_progress * 100/progress.total_size\n",
        "      sys.stdout.write(\"[%-100s] %d%%\" % ('='*int(percentage_done), int(percentage_done)))\n",
        "  downloaded.seek(0)\n",
        "  return downloaded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mT_Drig0glhE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "bdf06552-a28a-4d91-c66c-cdfba558255b"
      },
      "cell_type": "code",
      "source": [
        "parent_folder = get_parent_folder('av-sa')\n",
        "print(parent_folder)\n",
        "parent_folder[0][\"id\"]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'name': 'av-sa', 'id': '1uOG8csXYChpdzul_0v5lPgY-N5ssebG5'}]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1uOG8csXYChpdzul_0v5lPgY-N5ssebG5'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "W5OKyYNygoXj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "59143671-1a02-48de-c816-a8ef408f400b"
      },
      "cell_type": "code",
      "source": [
        "input_file_meta = get_files_from_parent(parent_folder[0][\"id\"])\n",
        "print(input_file_meta)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'test_tweets_anuFYb8.csv': '1DXtnnPd4THLQj6kUAU2JODxaQd3r1EXh', 'train_E6oV3lV.csv': '1FO1N6elMtXjPAh1Sz6BHojgHS-tqM8U3'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MRAfaCThgxYb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "c8872403-4e41-4d59-d1a3-e1aefa3433e2"
      },
      "cell_type": "code",
      "source": [
        "for file, id in input_file_meta.items():\n",
        "  downloaded = get_file_buffer(id, verbose=1)\n",
        "  dest_file = os.path.join(SOURCE_FOLDER, file)\n",
        "  print(\"processing %s data\" % file)\n",
        "  with open(dest_file, \"wb\") as out:\n",
        "    out.write(downloaded.read())\n",
        "    print(\"Done %s\" % dest_file)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r[====================================================================================================] 100%processing test_tweets_anuFYb8.csv data\n",
            "Done test_tweets_anuFYb8.csv\n",
            "[====================================================================================================] 100%processing train_E6oV3lV.csv data\n",
            "Done train_E6oV3lV.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dapH7p53g2Cy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train=pd.read_csv(os.path.join(SOURCE_FOLDER,'train_E6oV3lV.csv'))\n",
        "test=pd.read_csv(os.path.join(SOURCE_FOLDER,'test_tweets_anuFYb8.csv'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BvDUkNmwg9sa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def remove_pattern(input_txt, pattern):\n",
        "    r = re.findall(pattern, input_txt)\n",
        "    for i in r:\n",
        "        input_txt = re.sub(i, '', input_txt)\n",
        "        \n",
        "    return input_txt   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UShjggtPhhoV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 898
        },
        "outputId": "ba0514ab-c8ea-4d26-ded3-4209e1a020a6"
      },
      "cell_type": "code",
      "source": [
        "!pip install wordcloud\n",
        "!pip install gensim\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wordcloud\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/af/849edf14d573eba9c8082db898ff0d090428d9485371cc4fe21a66717ad2/wordcloud-1.5.0-cp36-cp36m-manylinux1_x86_64.whl (361kB)\n",
            "\u001b[K    100% |████████████████████████████████| 368kB 4.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from wordcloud) (4.0.0)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from wordcloud) (1.14.5)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->wordcloud) (0.45.1)\n",
            "Installing collected packages: wordcloud\n",
            "Successfully installed wordcloud-1.5.0\n",
            "Collecting gensim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/f3/37504f07651330ddfdefa631ca5246974a60d0908216539efda842fd080f/gensim-3.5.0-cp36-cp36m-manylinux1_x86_64.whl (23.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 23.5MB 989kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.5)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (0.19.1)\n",
            "Collecting smart-open>=1.2.1 (from gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/cf/3d/5f3a9a296d0ba8e00e263a8dee76762076b9eb5ddc254ccaa834651c8d65/smart_open-1.6.0.tar.gz\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.0)\n",
            "Collecting boto>=2.32 (from smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/10/c0b78c27298029e4454a472a1919bde20cb182dab1662cec7f2ca1dcc523/boto-2.49.0-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.4MB 9.9MB/s \n",
            "\u001b[?25hCollecting bz2file (from smart-open>=1.2.1->gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.18.4)\n",
            "Collecting boto3 (from smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/bf/e287410bc16b38b3e02853b7ca8af0f602a5075af5c8c77f50ec5339f034/boto3-1.9.0-py2.py3-none-any.whl (128kB)\n",
            "\u001b[K    100% |████████████████████████████████| 133kB 28.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2018.8.24)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.22)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.6)\n",
            "Collecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.2.1->gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/b7/31/05c8d001f7f87f0f07289a5fc0fc3832e9a57f2dbd4d3b0fee70e0d51365/jmespath-0.9.3-py2.py3-none-any.whl\n",
            "Collecting s3transfer<0.2.0,>=0.1.10 (from boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/14/2a0004d487464d120c9fb85313a75cd3d71a7506955be458eebfe19a6b1d/s3transfer-0.1.13-py2.py3-none-any.whl (59kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 23.4MB/s \n",
            "\u001b[?25hCollecting botocore<1.13.0,>=1.12.0 (from boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/3f/fecf78a1f4a495531eaab3cb386a33a32ae26371261bf39e030003dc05de/botocore-1.12.0-py2.py3-none-any.whl (4.7MB)\n",
            "\u001b[K    100% |████████████████████████████████| 4.7MB 930kB/s \n",
            "\u001b[?25hCollecting docutils>=0.10 (from botocore<1.13.0,>=1.12.0->boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/fa/08e9e6e0e3cbd1d362c3bbee8d01d0aedb2155c4ac112b19ef3cae8eed8d/docutils-0.14-py3-none-any.whl (543kB)\n",
            "\u001b[K    100% |████████████████████████████████| 552kB 24.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.0->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n",
            "Building wheels for collected packages: smart-open, bz2file\n",
            "  Running setup.py bdist_wheel for smart-open ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/73/f1/9b/ccf93d4ba073b6f79b1ed9df68ab5ce048d8136d0efcf90b30\n",
            "  Running setup.py bdist_wheel for bz2file ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/81/75/d6/e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n",
            "Successfully built smart-open bz2file\n",
            "Installing collected packages: boto, bz2file, jmespath, docutils, botocore, s3transfer, boto3, smart-open, gensim\n",
            "Successfully installed boto-2.49.0 boto3-1.9.0 botocore-1.12.0 bz2file-0.98 docutils-0.14 gensim-3.5.0 jmespath-0.9.3 s3transfer-0.1.13 smart-open-1.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gV-9u5gthdNv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns\n",
        "import string\n",
        "import nltk\n",
        "import warnings \n",
        "import gensim\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "from gensim.test.utils import common_texts, get_tmpfile\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument, LabeledSentence\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yyOejIt-g_jO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# remove twitter handles (@user)\n",
        "# @[\\w]* -  a regular expression which will pick any word starting with ‘@\n",
        "\n",
        "train['tidy_tweet'] = np.vectorize(remove_pattern)(train['tweet'], \"@[\\w]*\")\n",
        "test['tidy_tweet'] = np.vectorize(remove_pattern)(test['tweet'], \"@[\\w]*\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2tqtzquziGyu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# remove special characters, numbers, punctuations\n",
        "# replace everything except characters and hashtags with spaces\n",
        "\n",
        "#train['tidy_tweet'] = train['tidy_tweet'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
        "#test['tidy_tweet'] = test['tidy_tweet'].str.replace(\"[^a-zA-Z#]\", \" \")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zuKAb1zXhh9j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "## example ## \n",
        "re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])\", \" \",\"ouch...junior is angryð#got7 #junior #yugyo..., @user\")\n",
        "def process_tweet(tweet):\n",
        "    return \" \".join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])\", \" \",tweet.lower()).split())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TA47L9BdhnD1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train['tidy_tweet'] = train['tidy_tweet'].apply(process_tweet)\n",
        "test['tidy_tweet'] = test['tidy_tweet'].apply(process_tweet)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dLhQ-BEUiOab",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#remove short words like : oh, hm etc\n",
        "\n",
        "train['tidy_tweet'] = train['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
        "test['tidy_tweet'] = test['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wlj5VuMgiXg1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "949db1c7-384b-46b2-ed97-63aa7015c3a6"
      },
      "cell_type": "code",
      "source": [
        "# splitting a string of text into tokens(individual terms or words)\n",
        "\n",
        "train_tokenized_tweet = train['tidy_tweet'].apply(lambda x: x.split())\n",
        "train_tokenized_tweet.head()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [when, father, dysfunctional, selfish, drags, ...\n",
              "1    [thanks, lyft, credit, cause, they, offer, whe...\n",
              "2                              [bihday, your, majesty]\n",
              "3                      [model, love, take, with, time]\n",
              "4                    [factsguide, society, motivation]\n",
              "Name: tidy_tweet, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "4d4Fqqg5ifcz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "ea717988-5396-4f12-8350-e53bd8759ec9"
      },
      "cell_type": "code",
      "source": [
        "# splitting a string of text into tokens(individual terms or words)\n",
        "\n",
        "test_tokenized_tweet = test['tidy_tweet'].apply(lambda x: x.split())\n",
        "test_tokenized_tweet.head()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [studiolife, aislife, requires, passion, dedic...\n",
              "1    [white, supremacists, want, everyone, birds, m...\n",
              "2    [safe, ways, heal, your, acne, altwaystoheal, ...\n",
              "3    [cursed, child, book, reservations, already, w...\n",
              "4    [bihday, amazing, hilarious, nephew, ahmir, un...\n",
              "Name: tidy_tweet, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "hcRaz6ikimck",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Stemming is a rule-based process of stripping the suffixes (“ing”, “ly”, “es”, “s” etc)\n",
        "\n",
        "from nltk.stem.porter import *\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "train_tokenized_tweet = train_tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
        "test_tokenized_tweet = test_tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nz5E8pw6izp4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#combine tokens\n",
        "for i in range(len(train_tokenized_tweet)):\n",
        " train_tokenized_tweet[i] = \",\".join(train_tokenized_tweet[i])\n",
        "train['tidy_tweet'] = train_tokenized_tweet\n",
        "\n",
        "for i in range(len(test_tokenized_tweet)):\n",
        " test_tokenized_tweet[i] = \",\".join(test_tokenized_tweet[i])\n",
        "test['tidy_tweet'] = test_tokenized_tweet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j2QpKzlYcuOk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(train[\"tidy_tweet\"],train[\"label\"], test_size = 0.2, random_state = 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v-4PZQRjdaIz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import cross_val_score\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EUHqGrh3XR7o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english')), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB())])\n",
        "                     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mS8v9aYqYMJ2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "text_clf = text_clf.fit(train[\"tidy_tweet\"], train[\"label\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GD9WlGWSYYcc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],'tfidf__use_idf': (True, False),'clf__alpha': (1e-2, 1e-3)}\n",
        "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
        "gs_clf = gs_clf.fit(train[\"tidy_tweet\"], train[\"label\"])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mS-VThCRY5xU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix,f1_score\n",
        "prediction = gs_clf.predict(test[\"tidy_tweet\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9FAUybGObtdB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1d30246d-a2d6-4638-fb38-e912c0f8aaa6"
      },
      "cell_type": "code",
      "source": [
        "prediction"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "metadata": {
        "id": "1ZTaUyl4TOhw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test['label'] = prediction\n",
        "submission = test[['id','label']]\n",
        "my_sub = os.path.join(SOURCE_FOLDER, \"sub_tfdif.csv\")\n",
        "submission.to_csv(my_sub, index=False) # writing data to a CSV file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dG4tFE78DTaM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download( \"sub_tfdif.csv\" )"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}